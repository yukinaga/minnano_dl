{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "02_gradient_decent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/minnano_dl/blob/main/section_4/02_gradient_decent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otOny-9Bsd_G"
      },
      "source": [
        "# 勾配降下法\n",
        "勾配降下法では、関数の傾き（勾配）に基づき関数を最小化します。  \n",
        "ディープラーニングにおいて、出力と正解の誤差を最小化するために使われます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROgGoIJEsd_H"
      },
      "source": [
        "## 勾配降下法とは？\n",
        "「勾配降下法」は勾配法の一種で、勾配に基づき最小値を探索します。\n",
        "\n",
        "こちらの多変数関数、$y=f(X)$の最小値を勾配降下法により探索する例を考えます。  \n",
        "\n",
        "$$f(X) = f(x_1,x_2,\\cdots, x_i,\\cdots, x_n)$$\n",
        "\n",
        "このとき、$X$の初期値を適当に決めた上で、以下のような式に基づき$X$の全ての要素を更新します。\n",
        "\n",
        "（式 1）\n",
        "$$ x_i \\leftarrow x_i-\\eta\\frac{\\partial f(X)}{\\partial x_i} $$\n",
        "\n",
        "ここで$\\eta$は学習係数と呼ばれる定数で、$x_i$の更新速度を決めます。  \n",
        "この式により、勾配$\\frac{\\partial f(X)}{\\partial x_i} $が大きいほど（傾きが大きいほど）、$x_i$の値は大きく変更されることになります。  \n",
        "これを$f(X)$が変化しなくなるまで（勾配が0になるまで）繰り返すことで、$f(X)$の最小値を求めます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adwbsTWBsd_I"
      },
      "source": [
        "## 勾配降下法の実装\n",
        "以下の簡単な一変数関数$f(x)$の最小値を、勾配降下法を使って求めます。  \n",
        "\n",
        "$$f(x) = x^2 - 2x$$\n",
        "\n",
        "この関数は、$x$の値が1のときに最小値$f(1) = -1$をとります。また、この関数を$x$で微分すると次の通りです。\n",
        "\n",
        "$$\\frac{d f(x)}{d x} = 2x-2$$\n",
        "\n",
        "一変数なので、偏微分ではなく通常の微分を使っています。    \n",
        "以下のコードは、上記の関数の最小値を、勾配降下法により求めます。  \n",
        "（式1）を使って20回$x$を更新し、その過程を最後にグラフで表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-JRfnmFsd_J"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_func(x):  # 最小値を求める関数\n",
        "    return x**2 - 2*x\n",
        "\n",
        "def grad_func(x):  # 導関数\n",
        "    return 2*x - 2\n",
        "\n",
        "eta = 0.1  # 学習係数\n",
        "x = 4.0  # xに初期値を設定\n",
        "record_x = []  # xの記録\n",
        "record_y = []  # yの記録\n",
        "for i in range(20):  # 20回xを更新する\n",
        "    y = my_func(x)\n",
        "    record_x.append(x)\n",
        "    record_y.append(y)\n",
        "    x -= eta * grad_func(x)  # （式1）\n",
        "\n",
        "x_f = np.linspace(-2, 4)  # 表示範囲\n",
        "y_f = my_func(x_f)  \n",
        "\n",
        "plt.plot(x_f, y_f, linestyle=\"dashed\")  # 関数を点線で描画\n",
        "plt.scatter(record_x, record_y)  # xとyの記録を点で表示\n",
        "\n",
        "plt.xlabel(\"x\", size=14)\n",
        "plt.ylabel(\"y\", size=14)\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "385wN7xisd_N"
      },
      "source": [
        "$x$の初期値は4ですが、そこから関数を滑り落ちるようにして最小値付近に到達しました。  \n",
        "次第に$x$の間隔は狭くなっており、勾配が小さくなるとともに$x$の更新量が小さくなることが確認できます。  \n",
        "  \n",
        "勾配降下法により求められる最小値は厳密な最小値ではありませんが、現実の問題を扱う際は関数の形状さえ分からないことが多いので、勾配降下法により最小値を少しずつ探索するアプローチは有効です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e17Jim1fsd_O"
      },
      "source": [
        "## 局所的な最小値\n",
        "最小値には、全体の最小値と局所的な最小値があります。  \n",
        "先程の例では関数が比較的単純なので、全体の最小値にあっさりとたどり着くことができました。  \n",
        "しかしながら、ニューラルネットワークは複雑な関数なので、局所的な最小値に捕獲されて全体の最小値にたどり着けないことがあります。  \n",
        "以下では、局所的な最小値の例を見ていきます。以下の関数$f(x)$の最小値を、勾配降下法を使って求めます。  \n",
        "\n",
        "$$f(x) = x^4 + 2x^3 -3x^2 - 2x$$\n",
        "\n",
        "この関数を$x$で微分すると次のようになります。\n",
        "\n",
        "$$\\frac{d f(x)}{d x} = 4x^3 + 6x^2 - 6x - 2$$\n",
        "\n",
        "以下のコードでは、上記の関数に勾配降下法を適用しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si07-irasd_O"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_func(x):  # 最小値を求める関数\n",
        "    return x**4 + 2*x**3 - 3*x**2 - 2*x\n",
        "\n",
        "def grad_func(x):  # 導関数\n",
        "    return 4*x**3 + 6*x**2 - 6*x - 2\n",
        "\n",
        "eta = 0.01  # 学習係数\n",
        "x = 1.6  # xに初期値を設定\n",
        "record_x = []  # xの記録\n",
        "record_y = []  # yの記録\n",
        "for i in range(20):  # 20回xを更新する\n",
        "    y = my_func(x)\n",
        "    record_x.append(x)\n",
        "    record_y.append(y)\n",
        "    x -= eta * grad_func(x)  # （式1）\n",
        "\n",
        "x_f = np.linspace(-2.8, 1.6)  # 表示範囲\n",
        "y_f = my_func(x_f)  \n",
        "\n",
        "plt.plot(x_f, y_f, linestyle=\"dashed\")  # 関数を点線で描画\n",
        "plt.scatter(record_x, record_y)  # xとyの記録を点で表示\n",
        "\n",
        "plt.xlabel(\"x\", size=14)\n",
        "plt.ylabel(\"y\", size=14)\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w86uitVRsd_R"
      },
      "source": [
        "関数の曲線が点線で表されていますが、左側の谷が全体の最小値で、右側の谷が局所的な最小値です。  \n",
        "上記のコードでは`x = 1.6`を初期値としましたが、右側の谷に捕獲されて抜け出せなくなってしまいます。  \n",
        "ディープラーニングにおいて、このような局所的な最小値に捕らわれてしまうのは深刻な問題です。  \n",
        "適切に初期値を設定したり、あるいはランダムな動きを導入したりして、このような問題への対策が行われています。  \n",
        "今回のケースでは、初期値が適切に設定されれば全体の最小値にたどり着くことができます。"
      ]
    }
  ]
}