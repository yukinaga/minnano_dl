{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf+c3+ZpLcPyDyI90TzSHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/minnano_dl/blob/main/section_5/04_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PEI59mRTLnr"
      },
      "source": [
        "# 学習するニューラルネットワーク\n",
        "学習可能なニューラルネットワークを実装します。  \n",
        "「Iris dataset」という多数の花のデータが格納されたデータセットを使用し、品種の分類ができるようにニューラルネットワークを訓練します。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ_Ah4jbpW3b"
      },
      "source": [
        "## Iris datasetの導入\n",
        "scikit-learnというライブラリからIris datasetを導入します。  \n",
        "Iris datasetは、150個、3品種のIrisの花のサイズからなるデータセットです。  \n",
        "今回は、この中の2品種、SetosaとVersicolorのがく（Sepal）の長さと幅を使います。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln7D1pxbr8Hc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "# Irisデータの読み込み\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# 各花のサイズ\n",
        "iris_data = iris.data\n",
        "# print(iris_data)\n",
        "# print(iris_data.shape)  # 形状\n",
        "\n",
        "# 散布図で表示\n",
        "st_data = iris_data[:50]  # Setosa\n",
        "vc_data = iris_data[50:100]  # Versicolor\n",
        "plt.scatter(st_data[:, 0], st_data[:, 1], label=\"Setosa\")  # Sepal lengthとSepal width\n",
        "plt.scatter(vc_data[:, 0], vc_data[:, 1], label=\"Versicolor\")  # Sepal lengthとSepal width\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Sepal length (cm)\")\n",
        "plt.ylabel(\"Sepal width (cm)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpBmr6FT1saP"
      },
      "source": [
        "## 出力層の実装\n",
        "出力層をクラスとして実装します。  \n",
        "誤差には二乗和誤差を、活性化関数にはシグモイド関数を使います。  \n",
        "重みとバイアスの更新には以下の式を使います。  \n",
        "\n",
        "$$ w_i \\leftarrow w_i-\\eta x_i\\delta $$\n",
        "$$ b \\leftarrow b-\\eta \\delta $$\n",
        "\n",
        "学習のためには、以下の式で各ニューロンごとの$\\delta$を求める必要があります。  \n",
        "\n",
        "$$ \\delta = \\frac{\\partial E}{\\partial u} = \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u} $$ \n",
        "\n",
        "今回は二乗和誤差を誤差として使用するので、$\\frac{\\partial E}{\\partial y}$は以下の形になります。  \n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial E}{\\partial y} & = \\frac{\\partial}{\\partial y}(\\frac{1}{2} \\sum_{k=1}^n(y_k-t_k)^2) \\\\\n",
        "& = \\frac{\\partial}{\\partial y}(\\frac{1}{2}(y_0-t_0)^2+\\frac{1}{2}(y_1-t_1)^2+\\cdots+\\frac{1}{2}(y-t)^2+\\cdots+\\frac{1}{2}(y_n-t_n)^2) \\\\\n",
        "& = y-t\n",
        "\\end{aligned} $$\n",
        "\n",
        "また、活性化関数にはシグモイド関数を使うので、$\\frac{\\partial y}{\\partial u} $はシグモイド関数の導関数の形になります。  \n",
        "\n",
        "$$\\frac{\\partial y}{\\partial u} = (1-y)y$$\n",
        "\n",
        "これらを使って$\\delta$を求め、重みとバイアスを更新します。  \n",
        "  \n",
        "そして、前の層（1つ入力に近い層）に渡すために、以下の値を計算しておきます。  \n",
        "\n",
        "$$ \\frac{\\partial E}{\\partial x_i} = \\sum_{k=1}^n\\delta_kw_{ik} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNaBky9rDmKv"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # グラフの表示に使用\n",
        "\n",
        "class OutputLayer():\n",
        "    def __init__(self, n, W, B):  # 初期化\n",
        "        self.params = [n, W, B]  # 各パラメータをまとめる\n",
        "\n",
        "    def neuron(self, x, w, b):  # ニューロンを表すメソッド\n",
        "        u = np.sum(x*w) + b\n",
        "        return 1/(1+np.exp(-u))\n",
        "\n",
        "    def delta(self, t):  # δを計算するメソッド\n",
        "        return (1-self.y)*self.y * (self.y-t)\n",
        "\n",
        "    def __call__(self, x):  # 順伝播\n",
        "        n, W, B = self.params  # 各パラメータを取り出す\n",
        "        self.x = x  # 他のメソッドで使用\n",
        "\n",
        "        self.y= np.zeros(n)  # 出力を格納する配列\n",
        "        for i in range(n):  # 各ニューロンごとに\n",
        "            w = W[i]  # 重み\n",
        "            b = B[i]  # バイアス\n",
        "            self.y[i] = self.neuron(x, w, b)\n",
        "        return self.y  # この層の出力\n",
        "\n",
        "    def backward(self, t, eta):  # 逆伝播 t: 正解 eta: 学習係数\n",
        "        n, W, B = self.params  # 各パラメータを取り出す\n",
        "        delta = self.delta(t)\n",
        "\n",
        "        grad_x = np.zeros_like(self.x)  # 1つ前の層に渡す値\n",
        "        for i in range(n):  # 各ニューロンごとに\n",
        "            grad_x += delta[i] * W[i]\n",
        "            W[i] -= eta * self.x * delta[i]  # 重みの更新\n",
        "            B[i] -= eta * delta[i]  # バイアスの更新\n",
        "\n",
        "        return grad_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TVJivinYNhY"
      },
      "source": [
        "## 中間層の実装\n",
        "中間層をクラスとして実装します。  \n",
        "活性化関数にはシグモイド関数を使います。  \n",
        "出力層と同様に、重みとバイアスの更新には以下の式を使います。    \n",
        "\n",
        "$$ w_i \\leftarrow w_i-\\eta x_i\\delta $$\n",
        "$$ b \\leftarrow b-\\eta \\delta $$\n",
        "\n",
        "学習のためには、以下の式で各ニューロンごとの$\\delta$を求める必要があります。  \n",
        "\n",
        "$$ \\delta = \\frac{\\partial E}{\\partial u} = \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u} $$ \n",
        "\n",
        "活性化関数にはシグモイド関数を使うので、$\\frac{\\partial y}{\\partial u} $はシグモイド関数の導関数の形になります。  \n",
        "\n",
        "$$\\frac{\\partial y}{\\partial u} = (1-y)y$$\n",
        "\n",
        "$\\frac{\\partial E}{\\partial y}$は次の層（1つ出力に近い層）から受け取ります。  \n",
        "\n",
        "これらを使って$\\delta$を求め、重みとバイアスを更新します。  \n",
        "  \n",
        "出力層と同様に、前の層（1つ入力に近い層）に渡すために、以下の値を計算しておきます。  \n",
        "\n",
        "$$ \\frac{\\partial E}{\\partial x_i} = \\sum_{k=1}^n\\delta_kw_{ik} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDMei4uF771P"
      },
      "source": [
        "class MiddleLayer():\n",
        "    def __init__(self, n, W, B):  # 初期化\n",
        "        self.params = [n, W, B]  # 各パラメータをまとめる\n",
        "\n",
        "    def neuron(self, x, w, b):  # ニューロンを表すメソッド\n",
        "        u = np.sum(x*w) + b\n",
        "        return 1/(1+np.exp(-u))\n",
        "\n",
        "    def delta(self, grad_y):  # δを計算するメソッド\n",
        "        return (1-self.y)*self.y * grad_y\n",
        "\n",
        "    def __call__(self, x):  # 順伝播\n",
        "        n, W, B = self.params  # 各パラメータを取り出す\n",
        "        self.x = x  # 他のメソッドで使用\n",
        "\n",
        "        self.y= np.zeros(n)  # 出力を格納する配列\n",
        "        for i in range(n):  # 各ニューロンごとに\n",
        "            w = W[i]  # 重み\n",
        "            b = B[i]  # バイアス\n",
        "            self.y[i] = self.neuron(x, w, b)\n",
        "        return self.y  # この層の出力\n",
        "\n",
        "    def backward(self, grad_y, eta):  # 逆伝播 grad_y: ∂E/∂y eta: 学習係数\n",
        "        n, W, B = self.params  # 各パラメータを取り出す\n",
        "        delta = self.delta(grad_y)\n",
        "\n",
        "        grad_x = np.zeros_like(self.x)  # 1つ前の層に渡す値\n",
        "        for i in range(n):  # 各ニューロンごとに\n",
        "            grad_x += delta[i] * W[i]\n",
        "            W[i] -= eta * self.x * delta[i]  # 重みの更新\n",
        "            B[i] -= eta * delta[i]  # バイアスの更新\n",
        "\n",
        "        return grad_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQPTGunciTu"
      },
      "source": [
        "## ニューラルネットワークの訓練\n",
        "`OutputLayer`クラスと`MiddleLayer`クラスを使い、入力層、中間層、中間層、出力層が並んだニューラルネットワークを構築します。  \n",
        "そして、パラメータの更新を何度も繰り返し、ニューラルネットワークを訓練します。  \n",
        "学習が進むとともに、次第に正しくIrisの品種分類ができるようになることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUGi-dDxa84C"
      },
      "source": [
        "import random\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "iris_data = iris.data\n",
        "sl_data = iris_data[:100, 0] # SetosaとVersicolor、Sepal length\n",
        "sw_data = iris_data[:100, 1] # SetosaとVersicolor、Sepal width\n",
        "\n",
        "# 平均値を0に\n",
        "sl_ave = np.average(sl_data)  # 平均値\n",
        "sl_data -= sl_ave  # 平均値を引く\n",
        "sw_ave = np.average(sw_data)\n",
        "sw_data -= sw_ave\n",
        "\n",
        "# 入力をリストに格納\n",
        "train_data = []\n",
        "for i in range(100):\n",
        "    train_data.append([sl_data[i], sw_data[i], iris.target[i]])  # 入力1、入力2、正解\n",
        "\n",
        "# -- 各層の初期化 --  ニューロン数、重み、バイアスの初期値を設定\n",
        "layers = [MiddleLayer(2, np.array([[4.0, 4.0], [4.0, 4.0]]), np.array([2.0, -2.0])),\n",
        "          MiddleLayer(2, np.array([[4.0, 4.0], [4.0, 4.0]]), np.array([2.0, -2.0])),\n",
        "          OutputLayer(1, np.array([[1.0, -1.0]]), np.array([-0.5]))]\n",
        "\n",
        "# -- 順伝播 --\n",
        "def forward_propagation(x):\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "    return x\n",
        "\n",
        "# -- 逆伝播 --\n",
        "def backpropagation(t, eta):\n",
        "    grad_y = t\n",
        "    for layer in reversed(layers):  # 逆向き\n",
        "        grad_y = layer.backward(grad_y, eta)\n",
        "    return grad_y\n",
        "\n",
        "# グラフ表示用の関数\n",
        "def show_graph(epoch):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # 実行\n",
        "    st_predicted = [[], []]  # Setosa\n",
        "    vc_predicted = [[], []]  # Versicolor\n",
        "    for data in train_data:\n",
        "        x = np.array(data[:2])\n",
        "        if forward_propagation(x) < 0.5:\n",
        "            st_predicted[0].append(x[0]+sl_ave)\n",
        "            st_predicted[1].append(x[1]+sw_ave)\n",
        "        else:\n",
        "            vc_predicted[0].append(x[0]+sl_ave)\n",
        "            vc_predicted[1].append(x[1]+sw_ave)\n",
        "\n",
        "    # 分類結果をグラフ表示\n",
        "    plt.scatter(st_predicted[0], st_predicted[1], label=\"Setosa\")\n",
        "    plt.scatter(vc_predicted[0], vc_predicted[1], label=\"Versicolor\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.xlabel(\"Sepal length (cm)\")\n",
        "    plt.ylabel(\"Sepal width (cm)\")\n",
        "    plt.show()\n",
        "\n",
        "show_graph(0)\n",
        "\n",
        "# 学習と結果の表示\n",
        "eta = 0.3  # 学習係数\n",
        "for t in range(0, 64):  # 64回訓練\n",
        "    random.shuffle(train_data)\n",
        "    for data in train_data:\n",
        "        data = np.array(data)\n",
        "        forward_propagation(data[:2])  # 順伝播\n",
        "        backpropagation(data[2], eta)  # 逆伝播\n",
        "    if t+1 in [1, 2, 4, 8, 16, 32, 64]:  # グラフを表示するタイミング\n",
        "        show_graph(t+1)\n",
        "\n",
        "# 比較用に元の分類を散布図で表示\n",
        "st_data = iris_data[:50]  # Setosa\n",
        "vc_data = iris_data[50:100]  # Versicolor\n",
        "plt.scatter(st_data[:, 0], st_data[:, 1], label=\"Setosa\")\n",
        "plt.scatter(vc_data[:, 0], vc_data[:, 1], label=\"Versicolor\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Sepal length (cm)\")\n",
        "plt.ylabel(\"Sepal width (cm)\")\n",
        "plt.title(\"Original\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}