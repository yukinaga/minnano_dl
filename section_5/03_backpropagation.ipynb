{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcQP25e4Eki+P13v8Z+z20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/minnano_dl/blob/main/section_5/03_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkpKq-0PcHSU"
      },
      "source": [
        "# バックプロパゲーションの原理\n",
        "ニューラルネットワークが学習するのに必要なアルゴリズム「バックプロパゲーション」について学びましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0plBN3mEdQ"
      },
      "source": [
        "## 順伝播と合成関数\n",
        "\n",
        "ニューラルネットワークにおける順伝播は、中間層、出力層ともに以下の式で表されます。  \n",
        "\n",
        "$$u = \\sum_{k=1}^nw_kx_k + b$$\n",
        "$$y = f(u)$$\n",
        "\n",
        "上記の式において、$n$は1つのニューロンへの入力数、$x_k$と$w_k$は入力とそれに対応する重み、$b$はバイアス、$f$は活性化関数、$y$はニューロンの出力です。  \n",
        "  \n",
        "また、この式は以下のような合成関数と考えることもできます。  \n",
        "\n",
        "$$ y = f(u)$$\n",
        "$$u = g(w_1, w_2, \\cdots, w_n, x_1, x_2, \\cdots, x_n, b)$$\n",
        "\n",
        "ニューラルネットワークを合成関数と捉えることで、連鎖律を適用することが可能になります。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ78gycnhA1B"
      },
      "source": [
        "## 重みの更新\n",
        "\n",
        "勾配降下法を使い、誤差が小さくなるように重みを調整します。  \n",
        "以下は、最もシンプルな重みの更新式です。\n",
        "\n",
        "$$ w_i \\leftarrow w_i-\\eta\\frac{\\partial E}{\\partial w_i} $$\n",
        "\n",
        "ここで、重みの勾配$\\frac{\\partial E}{\\partial w_i}$は、連鎖律を用いて以下のように展開できます。\n",
        "\n",
        "（式1）\n",
        "$$ \\frac{\\partial E}{\\partial w_i}=\\frac{\\partial E}{\\partial u}\\frac{\\partial u}{\\partial w_i} $$\n",
        "\n",
        "ここで、右辺の$\\frac{\\partial u}{\\partial w_i}$の部分は、以下のように求めることができます。\n",
        "\n",
        "（式2）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u}{\\partial w_i} & = \\frac{\\partial (\\sum\\limits_{k=1}^n x_k w_k + b)}{\\partial w_k} \\\\\n",
        "& = \\frac{\\partial}{\\partial w_i}(x_1 w_{1}+x_2w_{2}+\\cdots +x_iw_i+\\cdots + x_nw_n + b) \\\\\n",
        "& = x_i\n",
        "\\end{aligned} $$\n",
        "\n",
        "偏微分した結果、$w_i$がかかっている項以外は全て消え、$x_i$のみが残ります。  \n",
        "ここで、以下のように$\\delta$を設定します。\n",
        "\n",
        "（式3）\n",
        "$$ \\delta = \\frac{\\partial E}{\\partial u} $$ \n",
        "\n",
        "（式2）（式3）により、（式1）は以下の形になります。  \n",
        "\n",
        "（式 4）\n",
        "$$ \\frac{\\partial E}{\\partial w_i} = x_i\\delta $$\n",
        "\n",
        "$\\delta$の求め方は層により異なります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iUdxYXUnblv"
      },
      "source": [
        "## バイアスの更新\n",
        "\n",
        "重みと同様に勾配降下法を使い、誤差が小さくなるようにバイアスを調整します。  \n",
        "以下は、最もシンプルなバイアスの更新の式です。\n",
        "\n",
        "$$ b \\leftarrow b-\\eta\\frac{\\partial E}{\\partial b} $$\n",
        "\n",
        "ここで、バイアスの勾配$\\frac{\\partial E}{\\partial b}$は、連鎖律を用いて以下のように展開できます。\n",
        "\n",
        "（式5）\n",
        "$$ \\frac{\\partial E}{\\partial b}=\\frac{\\partial E}{\\partial u}\\frac{\\partial u}{\\partial b} $$\n",
        "\n",
        "ここで、右辺の$\\frac{\\partial u}{\\partial w_i}$の部分は、以下のように求めることができます。\n",
        "\n",
        "（式6）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u}{\\partial b} & = \\frac{\\partial (\\sum\\limits_{k=1}^n x_k w_k + b)}{\\partial w_k} \\\\\n",
        "& = \\frac{\\partial}{\\partial b}(x_1 w_{1}+x_2w_{2}+\\cdots +x_iw_i+\\cdots + x_nw_n + b) \\\\\n",
        "& = 1\n",
        "\\end{aligned} $$\n",
        "\n",
        "偏微分した結果、バイアスの項以外は全て消え、1のみが残ります。  \n",
        "ここで、重みと同様に以下のように$\\delta$を設定します。\n",
        "\n",
        "（式7）\n",
        "$$ \\delta = \\frac{\\partial E}{\\partial u} $$ \n",
        "\n",
        "（式6）（式7）により、（式5）は以下の形になります。  \n",
        "\n",
        "（式 8）\n",
        "$$ \\frac{\\partial E}{\\partial b} = \\delta $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mziu62TipDBN"
      },
      "source": [
        "## $\\delta$の求め方\n",
        "\n",
        "$\\delta$は、連鎖律を使い以下のように展開することができます。  \n",
        "\n",
        "（式9）\n",
        "$$ \\delta = \\frac{\\partial E}{\\partial u} = \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u} $$ \n",
        "\n",
        "### 出力層\n",
        "出力層において、$\\frac{\\partial E}{\\partial y}$は誤差を出力で偏微分することで、$\\frac{\\partial y}{\\partial u}$は活性化関数を偏微分することで求めることができます。  \n",
        "\n",
        "### 中間層\n",
        "中間層では、$\\frac{\\partial y}{\\partial u}$は出力層と同様に活性化関数を偏微分することで求めることができます。   \n",
        "  \n",
        "中間層における$\\frac{\\partial E}{\\partial y}$は、求めるのに、次の層（この層よりも一つだけ出力に近い層）の値が必要になります。  \n",
        "次の層の変数には、目印として右肩に$(nl)$をつけます。  \n",
        "$nl$はnext layerの略です。  \n",
        "\n",
        "$y$は次の層の全てのニューロンへの入力となります。  \n",
        "従って、拡張された連鎖律を以下のように適用することができます。  \n",
        "  \n",
        "（式 10）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial E}{\\partial y} & = \\sum_{j=1}^m\\frac{\\partial E}{\\partial u_j^{(nl)}}\\frac{\\partial u_j^{(nl)}}{\\partial y} \\\\\n",
        "\\end{aligned} $$\n",
        "\n",
        "ここで、$m$は次の層のニューロン数で、$u_j^{(nl)}$は、次の層の各ニューロンにおける$u$の値です。  \n",
        "次の層の全てのニューロンで、\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial u_j^{(nl)}}\\frac{\\partial u_j^{(nl)}}{\\partial y}$$\n",
        "\n",
        "を計算し足し合わせることで、$\\frac{\\partial E}{\\partial y}$を求めることができます。  \n",
        "上記の$\\frac{\\partial E}{\\partial u_j^{(nl)}}$に関してですが、（式3）（式7）の$\\delta$で表すことができます。  \n",
        "\n",
        "（式11）\n",
        "$$ \\delta_j^{(nl)} = \\frac{\\partial E}{\\partial u_j^{(nl)}} $$\n",
        "\n",
        "また、$\\frac{\\partial u_j^{(nl)}}{\\partial y}$ですが、$y$はこのニューロンへの入力の1つであり、偏微分の結果この入力にかける重みのみ残ります。  \n",
        "従って、この$y$にかける重みを$w_{j}^{(nl)}$とすると、$\\frac{\\partial u_j^{(nl)}}{\\partial y}$は以下の通りになります。\n",
        "\n",
        "（式12）\n",
        "$$\\frac{\\partial u_j^{(nl)}}{\\partial y} = w_j^{(nl)}$$\n",
        "\n",
        "（式11）（式12）により、（式10）は以下の形になります。\n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial E}{\\partial y} & = \\sum_{j=1}^m\\delta_j^{(nl)}w_j^{(nl)} \\\\\n",
        "\\end{aligned} $$\n",
        "\n",
        "中間層では、この式により$\\frac{\\partial E}{\\partial y}$を求めることができます。  \n",
        "ここで、$w_j^{(nl)}$は次の層において$y$にかける重みです。  \n",
        "以上のように、中間層におけるニューロンの$\\delta$を求めるためには、次の層における$\\delta^{(nl)}$、及び$y$にかける重みを利用します。  \n",
        "これは、情報が出力から入力に向かって遡ることを意味します。  \n",
        "\n",
        "以上の逆伝播のアルゴリズムは、「バックプロパゲーション」（誤差逆伝播法）と呼ばれます。  \n",
        "バックプロパゲーションを使えば、層の数が増えても出力層 → 中間層 → 中間層 →... のように層を遡って重みやバイアスを更新することが可能になります。  "
      ]
    }
  ]
}